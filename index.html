<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Chain-Pole Self-Training Simulation (Stacked Pendulums)</title>
  <!-- Load TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <style>
    body {
      margin: 0;
      background: #222;
      color: #ddd;
      font-family: monospace;
    }
    #configPanel {
      padding: 10px;
      text-align: center;
      background: #333;
    }
    #configPanel label,
    #configPanel select,
    #configPanel button {
      font-size: 16px;
      margin: 0 5px;
    }
    canvas {
      background: #eee;
      display: block;
      margin: 10px auto;
    }
    #status {
      text-align: center;
      margin-top: 5px;
    }
  </style>
</head>
<body>
  <!-- Configuration panel: choose how many links (poles) to stack -->
  <div id="configPanel">
    <label for="numPolesSelect">Select Number of Poles (Stacked):</label>
    <select id="numPolesSelect">
      <option value="1">1 Pole</option>
      <option value="2">2 Poles</option>
      <option value="3" selected>3 Poles</option>
    </select>
    <button id="applyConfig">Apply Configuration</button>
  </div>
  
  <canvas id="simulationCanvas" width="800" height="600"></canvas>
  <div id="status">Initializing self-training simulation...</div>
  
  <script>
    /*******************************************************
     * Utility Vector Math Functions
     *******************************************************/
    function vectorAdd(v1, v2) {
      return v1.map((val, i) => val + v2[i]);
    }
    function vectorScale(v, scalar) {
      return v.map(val => val * scalar);
    }
    function vectorAddMultiple(...vectors) {
      let result = vectors[0].slice();
      for (let i = 1; i < vectors.length; i++) {
        result = result.map((val, j) => val + vectors[i][j]);
      }
      return result;
    }
    
    /*******************************************************
     * ChainPoleSimulator Class
     *
     * This simulator models a cart with a chain of pendulums
     * (stacked one above the other). The dynamics are defined
     * by an approximate cascade: the cart’s horizontal
     * acceleration is computed simply from the applied force,
     * then each link’s angular acceleration is computed using:
     *
     *    theta_i_ddot = - (g/L)* sin(theta_i) - (a_eff / L)* cos(theta_i)
     *
     * where for the first link a_eff is the cart acceleration,
     * and for each subsequent link a_eff is updated as:
     *
     *    a_eff = a_eff + L * theta_i_ddot * cos(theta_i) - L * theta_i_dot^2 * sin(theta_i)
     *
     * The state vector is:
     *    [ x, x_dot, theta_1, theta_1_dot, theta_2, theta_2_dot, … ]
     *
     * Zero angle means the link is perfectly vertical.
     *******************************************************/
    class ChainPoleSimulator {
      constructor(canvas, numPoles = 3) {
        this.canvas = canvas;
        this.ctx = canvas.getContext("2d");
        this.numPoles = numPoles;
        
        // Physics parameters.
        this.gravity = 9.8;       // m/s²
        this.dt = 0.02;           // time step in seconds
        this.massCart = 1.0;      // mass of the cart
        this.massPole = 0.1;      // mass of each link (for inertia estimation)
        this.L = 1.0;             // link length (in meters)
        
        // Our state is [x, x_dot, theta1, theta1_dot, theta2, theta2_dot, ...]
        this.state = new Float32Array(2 + 2 * this.numPoles);
        this.reset();
        
        // Termination thresholds – if the cart moves too far or any link deviates too much.
        this.xThreshold = 2.4;
        this.thetaThreshold = 15 * Math.PI / 180;  // 15° in radians
        
        // Graphics parameters.
        this.scale = 100;         // pixels per meter.
        this.cartWidth = 0.8 * this.scale;
        this.cartHeight = 0.4 * this.scale;
        this.groundY = this.canvas.height - 100;
      }
      
      reset() {
        // Reset cart state.
        this.state[0] = 0;  // x position.
        this.state[1] = 0;  // x velocity.
        // For each link, set small noise around zero so that links start nearly vertical.
        for (let i = 0; i < this.numPoles; i++) {
          this.state[2 + 2*i] = (Math.random() - 0.5) * 0.05;  // theta (radians), near 0.
          this.state[3 + 2*i] = (Math.random() - 0.5) * 0.05;  // angular velocity.
        }
      }
      
      isTerminal() {
        if (Math.abs(this.state[0]) > this.xThreshold) return true;
        for (let i = 0; i < this.numPoles; i++) {
          if (Math.abs(this.state[2 + 2*i]) > this.thetaThreshold) return true;
        }
        return false;
      }
      
      // Compute derivatives for the chain dynamics.
      computeDerivatives(state, force) {
        const deriv = new Float32Array(state.length);
        // Derivative for cart:
        deriv[0] = state[1];
        // For simplicity, assume the cart acceleration is:
        const totalMass = this.massCart + this.numPoles * this.massPole;
        const x_ddot = force / totalMass;
        deriv[1] = x_ddot;
        
        // Use a cascading approach. For the first link, effective acceleration is cart acceleration.
        let a_eff = x_ddot;
        
        // Loop over each link.
        for (let i = 0; i < this.numPoles; i++) {
          const idx = 2 + 2*i;
          const theta = state[idx];
          const theta_dot = state[idx+1];
          // d(theta)/dt = theta_dot.
          deriv[idx] = theta_dot;
          // Approximate angular acceleration:
          const theta_ddot = - (this.gravity / this.L) * Math.sin(theta)
                             - (a_eff / this.L) * Math.cos(theta);
          deriv[idx+1] = theta_ddot;
          // Update effective acceleration for the next link.
          a_eff = a_eff + this.L * theta_ddot * Math.cos(theta) - this.L * theta_dot * theta_dot * Math.sin(theta);
        }
        return deriv;
      }
      
      // Perform one Runge-Kutta 4th order integration step.
      rk4Step(force) {
        const dt = this.dt;
        const s = this.state;
        const k1 = this.computeDerivatives(s, force);
        const s2 = vectorAdd(s, vectorScale(k1, dt/2));
        const k2 = this.computeDerivatives(s2, force);
        const s3 = vectorAdd(s, vectorScale(k2, dt/2));
        const k3 = this.computeDerivatives(s3, force);
        const s4 = vectorAdd(s, vectorScale(k3, dt));
        const k4 = this.computeDerivatives(s4, force);
        const incr = vectorScale(vectorAddMultiple(k1, vectorScale(k2,2), vectorScale(k3,2), k4), dt/6);
        this.state = vectorAdd(s, incr);
      }
      
      // Update simulation over elapsed time (in milliseconds) using the applied force.
      update(force, elapsedTime) {
        const steps = Math.floor(elapsedTime / (this.dt * 1000));
        for (let i = 0; i < steps; i++) {
          this.rk4Step(force);
          if (this.isTerminal()) {
            this.reset();
          }
        }
      }
      
      // Render the cart and then the chain of poles.
      render() {
        const ctx = this.ctx;
        ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw ground.
        ctx.strokeStyle = "#888";
        ctx.lineWidth = 2;
        ctx.beginPath();
        ctx.moveTo(0, this.groundY + 10);
        ctx.lineTo(this.canvas.width, this.groundY + 10);
        ctx.stroke();
        
        // Draw cart.
        const cartX = this.canvas.width/2 + this.state[0] * this.scale;
        const cartY = this.groundY;
        ctx.fillStyle = "#555";
        ctx.fillRect(cartX - this.cartWidth/2, cartY - this.cartHeight, this.cartWidth, this.cartHeight);
        // Wheels.
        ctx.fillStyle = "#222";
        ctx.beginPath();
        ctx.arc(cartX - this.cartWidth/3, cartY, 10, 0, 2 * Math.PI);
        ctx.arc(cartX + this.cartWidth/3, cartY, 10, 0, 2 * Math.PI);
        ctx.fill();
        
        // Now draw the chain starting from the top center of the cart.
        let baseX = cartX;
        let baseY = cartY - this.cartHeight;
        for (let i = 0; i < this.numPoles; i++) {
          const theta = this.state[2 + 2*i];
          // Compute the end point of the link.
          const linkLengthPx = this.L * this.scale;
          const endX = baseX + linkLengthPx * Math.sin(theta);
          const endY = baseY - linkLengthPx * Math.cos(theta);
          // Draw link.
          ctx.strokeStyle = "hsl(" + (i * 120) + ", 70%, 50%)";
          ctx.lineWidth = 6;
          ctx.beginPath();
          ctx.moveTo(baseX, baseY);
          ctx.lineTo(endX, endY);
          ctx.stroke();
          // Draw joint.
          ctx.fillStyle = "hsl(" + (i * 120) + ", 70%, 50%)";
          ctx.beginPath();
          ctx.arc(baseX, baseY, 6, 0, 2*Math.PI);
          ctx.fill();
          // The end point becomes the base for the next link.
          baseX = endX;
          baseY = endY;
        }
      }
    }
    
    /*******************************************************
     * PPOAgent Class (Self-Training via TensorFlow.js)
     *
     * This is unchanged from previous examples except that
     * the observation dimension is now 2+2*N (for N links).
     *******************************************************/
    class PPOAgent {
      constructor(obs_dim, action_dim, hiddenSizes = [64, 64]) {
        this.obs_dim = obs_dim;
        this.action_dim = action_dim;
        this.clipRatio = 0.2;
        this.entropyCoef = 0.01;
        this.gamma = 0.99;
        this.lam = 0.95;
        this.actorLearningRate = 3e-4;
        this.criticLearningRate = 1e-3;
        
        // Build networks.
        this.actor = this.buildActor(obs_dim, action_dim, hiddenSizes);
        this.actorLogStd = tf.variable(tf.fill([action_dim], -0.5));
        this.critic = this.buildCritic(obs_dim, hiddenSizes);
        
        this.actorOptimizer = tf.train.adam(this.actorLearningRate);
        this.criticOptimizer = tf.train.adam(this.criticLearningRate);
      }
      
      buildActor(inputDim, outputDim, hiddenSizes) {
        const model = tf.sequential();
        model.add(tf.layers.dense({ units: hiddenSizes[0], activation: 'tanh', inputShape: [inputDim] }));
        for (let i = 1; i < hiddenSizes.length; i++) {
          model.add(tf.layers.dense({ units: hiddenSizes[i], activation: 'tanh' }));
        }
        model.add(tf.layers.dense({ units: outputDim }));
        return model;
      }
      
      buildCritic(inputDim, hiddenSizes) {
        const model = tf.sequential();
        model.add(tf.layers.dense({ units: hiddenSizes[0], activation: 'tanh', inputShape: [inputDim] }));
        for (let i = 1; i < hiddenSizes.length; i++) {
          model.add(tf.layers.dense({ units: hiddenSizes[i], activation: 'tanh' }));
        }
        model.add(tf.layers.dense({ units: 1 }));
        return model;
      }
      
      getAction(state) {
        return tf.tidy(() => {
          const obs = tf.tensor2d(state, [1, this.obs_dim]);
          const mu = this.actor.predict(obs);
          const std = tf.exp(this.actorLogStd);
          const noise = tf.randomNormal(mu.shape);
          const action = mu.add(std.mul(noise));
          let logProb = tf.mul(-0.5, tf.add(tf.square(noise),
                      tf.add(tf.mul(2, this.actorLogStd),
                             tf.log(tf.scalar(2 * Math.PI)))));
          logProb = tf.sum(logProb, 1);
          const value = this.critic.predict(obs);
          return {
            action: action.dataSync(),
            logProb: logProb.dataSync()[0],
            value: value.dataSync()[0]
          };
        });
      }
      
      computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch) {
        return tf.tidy(() => {
          const mu = this.actor.apply(obsBatch);
          const std = tf.exp(this.actorLogStd);
          const actionDiff = tf.div(tf.sub(actBatch, mu), std.add(1e-8));
          let logp = tf.mul(-0.5, tf.add(tf.square(actionDiff),
                          tf.add(tf.mul(2, this.actorLogStd), tf.log(tf.scalar(2 * Math.PI)))));
          logp = tf.sum(logp, 1);
          
          const ratio = tf.exp(tf.sub(logp, logpOldBatch));
          const clipAdv1 = ratio.mul(advBatch);
          const clipAdv2 = tf.clipByValue(ratio, 1 - this.clipRatio, 1 + this.clipRatio).mul(advBatch);
          const actorLoss = tf.neg(tf.mean(tf.minimum(clipAdv1, clipAdv2)));
          
          const entropy = tf.mean(tf.sum(tf.add(this.actorLogStd, tf.scalar(0.5 * Math.log(2 * Math.PI * Math.E))), 1));
          const totalActorLoss = actorLoss.sub(this.entropyCoef * entropy);
          
          const values = this.critic.apply(obsBatch);
          const criticLoss = tf.mean(tf.square(tf.sub(retBatch, values)));
          
          return { totalActorLoss, criticLoss };
        });
      }
      
      async update(observations, actions, advantages, returns, logpOld, epochs = 10, batchSize = 64) {
        const datasetSize = observations.shape[0];
        const indices = tf.util.createShuffledIndices(datasetSize);
        for (let epoch = 0; epoch < epochs; epoch++) {
          for (let i = 0; i < datasetSize; i += batchSize) {
            const batchIndices = indices.slice(i, i + batchSize);
            const obsBatch = tf.gather(observations, batchIndices);
            const actBatch = tf.gather(actions, batchIndices);
            const advBatch = tf.gather(advantages, batchIndices);
            const retBatch = tf.gather(returns, batchIndices);
            const logpOldBatch = tf.gather(logpOld, batchIndices);
            await this.actorOptimizer.minimize(() => {
              const losses = this.computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch);
              return losses.totalActorLoss;
            });
            await this.criticOptimizer.minimize(() => {
              const losses = this.computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch);
              return losses.criticLoss;
            });
            obsBatch.dispose(); actBatch.dispose(); advBatch.dispose(); retBatch.dispose(); logpOldBatch.dispose();
          }
        }
      }
      
      computeGAE(rewards, values, dones) {
        let adv = new Array(rewards.length);
        let lastgaelam = 0;
        for (let t = rewards.length - 1; t >= 0; t--) {
          const nextValue = t === rewards.length - 1 ? 0 : values[t+1];
          const nextNonTerminal = dones[t] ? 0 : 1;
          const delta = rewards[t] + this.gamma * nextValue * nextNonTerminal - values[t];
          lastgaelam = delta + this.gamma * this.lam * nextNonTerminal * lastgaelam;
          adv[t] = lastgaelam;
        }
        const returns = adv.map((a, idx) => a + values[idx]);
        const mean = adv.reduce((a, b) => a + b, 0) / adv.length;
        const std = Math.sqrt(adv.map(a => (a - mean) ** 2).reduce((a, b) => a + b, 0) / adv.length);
        adv = adv.map(a => (a - mean) / (std + 1e-8));
        return { advantages: adv, returns: returns };
      }
    }
    
    /*******************************************************
     * Global Setup and Reinitialization
     *******************************************************/
    const canvas = document.getElementById("simulationCanvas");
    const statusDiv = document.getElementById("status");
    
    let simulator;  // instance of ChainPoleSimulator
    let agent;      // PPOAgent instance
    let obs_dim;
    const action_dim = 1;
    
    let bufferObservations = [];
    let bufferActions = [];
    let bufferRewards = [];
    let bufferDones = [];
    let bufferLogProbs = [];
    let bufferValues = [];
    const batchSizeThreshold = 2048;  // timesteps for a training update
    
    function initSimulation() {
      const numPoles = parseInt(document.getElementById("numPolesSelect").value);
      simulator = new ChainPoleSimulator(canvas, numPoles);
      obs_dim = 2 + 2 * simulator.numPoles;
      agent = new PPOAgent(obs_dim, action_dim);
      bufferObservations = [];
      bufferActions = [];
      bufferRewards = [];
      bufferDones = [];
      bufferLogProbs = [];
      bufferValues = [];
      statusDiv.textContent = `Configuration applied: ${numPoles} Pole${numPoles > 1 ? "s (chain configuration)" : ""}`;
    }
    initSimulation();
    document.getElementById("applyConfig").addEventListener("click", initSimulation);
    
    /*******************************************************
     * Simulation & Training Loop
     *
     * The physics and training update run in a setInterval
     * (every ~20ms), while rendering uses requestAnimationFrame.
     *******************************************************/
    let lastSimTime = performance.now();
    
    setInterval(() => {
      const now = performance.now();
      const elapsed = now - lastSimTime;
      lastSimTime = now;
      
      // Get current state and select an action.
      const state = Array.from(simulator.state);
      const { action, logProb, value } = agent.getAction(state);
      const force = action[0];  // single continuous control
      simulator.update(force, elapsed);
      
      // For simplicity, use reward=1 per update.
      const reward = 1;
      const done = simulator.isTerminal() ? 1 : 0;
      if (done) simulator.reset();
      
      bufferObservations.push(state);
      bufferActions.push([force]);
      bufferRewards.push(reward);
      bufferDones.push(done);
      bufferLogProbs.push(logProb);
      bufferValues.push(value);
      
      if (bufferObservations.length >= batchSizeThreshold) {
        const gae = agent.computeGAE(bufferRewards, bufferValues, bufferDones);
        const obsTensor = tf.tensor2d(bufferObservations, [bufferObservations.length, obs_dim]);
        const actTensor = tf.tensor2d(bufferActions, [bufferActions.length, action_dim]);
        const advTensor = tf.tensor1d(gae.advantages);
        const retTensor = tf.tensor1d(gae.returns);
        const logpOldTensor = tf.tensor1d(bufferLogProbs);
        agent.update(obsTensor, actTensor, advTensor, retTensor, logpOldTensor)
             .then(() => {
               bufferObservations = [];
               bufferActions = [];
               bufferRewards = [];
               bufferDones = [];
               bufferLogProbs = [];
               bufferValues = [];
               obsTensor.dispose();
               actTensor.dispose();
               advTensor.dispose();
               retTensor.dispose();
               logpOldTensor.dispose();
               console.log("Training update completed.");
             });
      }
      
      statusDiv.textContent = `Collected Steps: ${bufferObservations.length} (Obs Dim: ${obs_dim})`;
    }, 20);
    
    function renderLoop() {
      simulator.render();
      requestAnimationFrame(renderLoop);
    }
    requestAnimationFrame(renderLoop);
  </script>
</body>
</html>

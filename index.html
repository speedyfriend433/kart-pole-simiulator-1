<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Chain-Pole Self-Training Simulation with Episode Reset & Trajectories</title>
  <!-- Load TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <style>
    body {
      margin: 0;
      background: #222;
      color: #ddd;
      font-family: monospace;
    }
    #configPanel {
      padding: 10px;
      text-align: center;
      background: #333;
    }
    #configPanel label,
    #configPanel select,
    #configPanel button {
      font-size: 16px;
      margin: 0 5px;
    }
    canvas {
      background: #eee;
      display: block;
      margin: 10px auto;
    }
    #status {
      text-align: center;
      margin-top: 5px;
    }
  </style>
</head>
<body>
  <!-- Configuration panel: choose how many links (poles) to stack -->
  <div id="configPanel">
    <label for="numPolesSelect">Select Number of Poles (Stacked):</label>
    <select id="numPolesSelect">
      <option value="1">1 Pole</option>
      <option value="2">2 Poles</option>
      <option value="3" selected>3 Poles</option>
    </select>
    <button id="applyConfig">Apply Configuration</button>
  </div>
  
  <canvas id="simulationCanvas" width="800" height="600"></canvas>
  <div id="status">Initializing self-training simulation...</div>
  
  <script>
    /*******************************************************
     * Utility Vector Math Functions
     *******************************************************/
    function vectorAdd(v1, v2) {
      return v1.map((val, i) => val + v2[i]);
    }
    function vectorScale(v, scalar) {
      return v.map(val => val * scalar);
    }
    function vectorAddMultiple(...vectors) {
      let result = vectors[0].slice();
      for (let i = 1; i < vectors.length; i++) {
        result = result.map((val, j) => val + vectors[i][j]);
      }
      return result;
    }
    
    /*******************************************************
     * ChainPoleSimulator Class
     *
     * This simulator models a cart with a chain of pendulums
     * (stacked one above the other). The dynamics are computed
     * approximately using a cascading equation. Zero angle means
     * vertical. In addition, this class maintains a trail (trajectory)
     * for each link’s tip.
     *******************************************************/
    class ChainPoleSimulator {
      constructor(canvas, numPoles = 3) {
        this.canvas = canvas;
        this.ctx = canvas.getContext("2d");
        this.numPoles = numPoles;
        
        // Physics parameters
        this.gravity = 9.8;       // m/s²
        this.dt = 0.02;           // time step in seconds
        this.massCart = 1.0;      // mass of the cart
        this.massPole = 0.1;      // mass for each link (for inertia estimation)
        this.L = 1.0;             // link length [m]
        
        // Our state is [x, x_dot, theta1, theta1_dot, theta2, theta2_dot, ...]
        this.state = new Float32Array(2 + 2 * this.numPoles);
        this.reset();
        
        // Termination thresholds
        this.xThreshold = 2.4;
        this.thetaThreshold = 15 * Math.PI / 180;  // 15° in radians
        
        // Graphics parameters
        this.scale = 100;         // pixels per meter
        this.cartWidth = 0.8 * this.scale;
        this.cartHeight = 0.4 * this.scale;
        this.groundY = this.canvas.height - 100;
        
        // For trajectory tracking, maintain an array of trails for each link.
        this.maxTrailLength = 100;  // maximum points to store per trail
        this.initializeTrails();
      }
      
      initializeTrails() {
        this.trails = [];
        for (let i = 0; i < this.numPoles; i++) {
          this.trails.push([]);
        }
      }
      
      reset() {
        // Reset cart state.
        this.state[0] = 0;  // x position.
        this.state[1] = 0;  // x velocity.
        // For each link, set a small noise around zero.
        for (let i = 0; i < this.numPoles; i++) {
          this.state[2 + 2*i] = (Math.random() - 0.5) * 0.05;  // theta (radians)
          this.state[3 + 2*i] = (Math.random() - 0.5) * 0.05;  // angular velocity.
        }
        this.initializeTrails();
      }
      
      isTerminal() {
        if (Math.abs(this.state[0]) > this.xThreshold) return true;
        for (let i = 0; i < this.numPoles; i++) {
          if (Math.abs(this.state[2 + 2*i]) > this.thetaThreshold) return true;
        }
        return false;
      }
      
      // Compute derivatives for chain dynamics.
      computeDerivatives(state, force) {
        const deriv = new Float32Array(state.length);
        // Cart dynamics:
        deriv[0] = state[1];
        const totalMass = this.massCart + this.numPoles * this.massPole;
        const x_ddot = force / totalMass;
        deriv[1] = x_ddot;
        
        // Cascading dynamics for each link.
        let a_eff = x_ddot;  // For the first link, effective acceleration equals the cart's.
        for (let i = 0; i < this.numPoles; i++) {
          const idx = 2 + 2*i;
          const theta = state[idx];
          const theta_dot = state[idx+1];
          deriv[idx] = theta_dot;
          const theta_ddot = - (this.gravity / this.L) * Math.sin(theta)
                             - (a_eff / this.L) * Math.cos(theta);
          deriv[idx+1] = theta_ddot;
          // Update effective acceleration for the next link.
          a_eff = a_eff + this.L * theta_ddot * Math.cos(theta) - this.L * theta_dot * theta_dot * Math.sin(theta);
        }
        return deriv;
      }
      
      // Perform a single RK4 integration step.
      rk4Step(force) {
        const dt = this.dt;
        const s = this.state;
        const k1 = this.computeDerivatives(s, force);
        const s2 = vectorAdd(s, vectorScale(k1, dt/2));
        const k2 = this.computeDerivatives(s2, force);
        const s3 = vectorAdd(s, vectorScale(k2, dt/2));
        const k3 = this.computeDerivatives(s3, force);
        const s4 = vectorAdd(s, vectorScale(k3, dt));
        const k4 = this.computeDerivatives(s4, force);
        const incr = vectorScale(vectorAddMultiple(k1, vectorScale(k2, 2), vectorScale(k3, 2), k4), dt/6);
        this.state = vectorAdd(s, incr);
      }
      
      // Update simulation over elapsed time in milliseconds.
      update(force, elapsedTime) {
        const steps = Math.floor(elapsedTime / (this.dt * 1000));
        for (let i = 0; i < steps; i++) {
          this.rk4Step(force);
          // Update trails after each simulation step.
          this.updateTrails();
          if (this.isTerminal()) {
            // When termination is reached, reset.
            this.reset();
            break;
          }
        }
      }
      
      // Compute the tip positions for each link (in canvas pixel coordinates).
      getLinkTipCoordinates() {
        const cartX = this.canvas.width/2 + this.state[0] * this.scale;
        const cartY = this.groundY;
        let baseX = cartX;
        let baseY = cartY - this.cartHeight;
        let tips = [];
        for (let i = 0; i < this.numPoles; i++) {
          const theta = this.state[2 + 2*i];
          const linkLengthPx = this.L * this.scale;
          const endX = baseX + linkLengthPx * Math.sin(theta);
          const endY = baseY - linkLengthPx * Math.cos(theta);
          tips.push({x: endX, y: endY});
          baseX = endX;
          baseY = endY;
        }
        return tips;
      }
      
      // Update the trajectory trails for each link.
      updateTrails() {
        const tips = this.getLinkTipCoordinates();
        for (let i = 0; i < this.numPoles; i++) {
          this.trails[i].push(tips[i]);
          if (this.trails[i].length > this.maxTrailLength) {
            this.trails[i].shift();
          }
        }
      }
      
      render() {
        const ctx = this.ctx;
        ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw ground.
        ctx.strokeStyle = "#888";
        ctx.lineWidth = 2;
        ctx.beginPath();
        ctx.moveTo(0, this.groundY + 10);
        ctx.lineTo(this.canvas.width, this.groundY + 10);
        ctx.stroke();
        
        // Draw cart.
        const cartX = this.canvas.width/2 + this.state[0] * this.scale;
        const cartY = this.groundY;
        ctx.fillStyle = "#555";
        ctx.fillRect(cartX - this.cartWidth/2, cartY - this.cartHeight, this.cartWidth, this.cartHeight);
        // Wheels.
        ctx.fillStyle = "#222";
        ctx.beginPath();
        ctx.arc(cartX - this.cartWidth/3, cartY, 10, 0, 2 * Math.PI);
        ctx.arc(cartX + this.cartWidth/3, cartY, 10, 0, 2 * Math.PI);
        ctx.fill();
        
        // Draw the chain of links.
        let baseX = cartX;
        let baseY = cartY - this.cartHeight;
        for (let i = 0; i < this.numPoles; i++) {
          const theta = this.state[2 + 2*i];
          const linkLengthPx = this.L * this.scale;
          const endX = baseX + linkLengthPx * Math.sin(theta);
          const endY = baseY - linkLengthPx * Math.cos(theta);
          ctx.strokeStyle = "hsl(" + (i * 120) + ", 70%, 50%)";
          ctx.lineWidth = 6;
          ctx.beginPath();
          ctx.moveTo(baseX, baseY);
          ctx.lineTo(endX, endY);
          ctx.stroke();
          // Draw joint
          ctx.fillStyle = "hsl(" + (i * 120) + ", 70%, 50%)";
          ctx.beginPath();
          ctx.arc(baseX, baseY, 6, 0, 2 * Math.PI);
          ctx.fill();
          baseX = endX;
          baseY = endY;
        }
        
        // Draw trails for each link.
        for (let i = 0; i < this.numPoles; i++) {
          const trail = this.trails[i];
          if (trail.length > 1) {
            ctx.strokeStyle = "rgba(0, 0, 0, 0.3)";  // semi-transparent black
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(trail[0].x, trail[0].y);
            for (let j = 1; j < trail.length; j++) {
              ctx.lineTo(trail[j].x, trail[j].y);
            }
            ctx.stroke();
          }
        }
      }
    }
    
    /*******************************************************
     * PPOAgent Class (Self-Training via TensorFlow.js)
     *
     * Implements an actor–critic model using PPO.
     * The actor outputs a continuous force and the critic
     * estimates state value.
     *******************************************************/
    class PPOAgent {
      constructor(obs_dim, action_dim, hiddenSizes = [64, 64]) {
        this.obs_dim = obs_dim;
        this.action_dim = action_dim;
        this.clipRatio = 0.2;
        this.entropyCoef = 0.01;
        this.gamma = 0.99;
        this.lam = 0.95;
        this.actorLearningRate = 3e-4;
        this.criticLearningRate = 1e-3;
        
        // Build networks.
        this.actor = this.buildActor(obs_dim, action_dim, hiddenSizes);
        this.actorLogStd = tf.variable(tf.fill([action_dim], -0.5));
        this.critic = this.buildCritic(obs_dim, hiddenSizes);
        
        this.actorOptimizer = tf.train.adam(this.actorLearningRate);
        this.criticOptimizer = tf.train.adam(this.criticLearningRate);
      }
      
      buildActor(inputDim, outputDim, hiddenSizes) {
        const model = tf.sequential();
        model.add(tf.layers.dense({ units: hiddenSizes[0], activation: 'tanh', inputShape: [inputDim] }));
        for (let i = 1; i < hiddenSizes.length; i++) {
          model.add(tf.layers.dense({ units: hiddenSizes[i], activation: 'tanh' }));
        }
        model.add(tf.layers.dense({ units: outputDim }));
        return model;
      }
      
      buildCritic(inputDim, hiddenSizes) {
        const model = tf.sequential();
        model.add(tf.layers.dense({ units: hiddenSizes[0], activation: 'tanh', inputShape: [inputDim] }));
        for (let i = 1; i < hiddenSizes.length; i++) {
          model.add(tf.layers.dense({ units: hiddenSizes[i], activation: 'tanh' }));
        }
        model.add(tf.layers.dense({ units: 1 }));
        return model;
      }
      
      getAction(state) {
        return tf.tidy(() => {
          const obs = tf.tensor2d(state, [1, this.obs_dim]);
          const mu = this.actor.predict(obs);
          const std = tf.exp(this.actorLogStd);
          const noise = tf.randomNormal(mu.shape);
          const action = mu.add(std.mul(noise));
          let logProb = tf.mul(-0.5, tf.add(tf.square(noise),
                      tf.add(tf.mul(2, this.actorLogStd), tf.log(tf.scalar(2 * Math.PI)))));
          logProb = tf.sum(logProb, 1);
          const value = this.critic.predict(obs);
          return {
            action: action.dataSync(),
            logProb: logProb.dataSync()[0],
            value: value.dataSync()[0]
          };
        });
      }
      
      computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch) {
        return tf.tidy(() => {
          const mu = this.actor.apply(obsBatch);
          const std = tf.exp(this.actorLogStd);
          const actionDiff = tf.div(tf.sub(actBatch, mu), std.add(1e-8));
          let logp = tf.mul(-0.5, tf.add(tf.square(actionDiff),
                          tf.add(tf.mul(2, this.actorLogStd), tf.log(tf.scalar(2 * Math.PI)))));
          logp = tf.sum(logp, 1);
          
          const ratio = tf.exp(tf.sub(logp, logpOldBatch));
          const clipAdv1 = ratio.mul(advBatch);
          const clipAdv2 = tf.clipByValue(ratio, 1 - this.clipRatio, 1 + this.clipRatio).mul(advBatch);
          const actorLoss = tf.neg(tf.mean(tf.minimum(clipAdv1, clipAdv2)));
          
          const entropy = tf.mean(tf.sum(tf.add(this.actorLogStd, tf.scalar(0.5 * Math.log(2 * Math.PI * Math.E))), 1));
          const totalActorLoss = actorLoss.sub(this.entropyCoef * entropy);
          
          const values = this.critic.apply(obsBatch);
          const criticLoss = tf.mean(tf.square(tf.sub(retBatch, values)));
          
          return { totalActorLoss, criticLoss };
        });
      }
      
      async update(observations, actions, advantages, returns, logpOld, epochs = 10, batchSize = 64) {
        const datasetSize = observations.shape[0];
        const indices = tf.util.createShuffledIndices(datasetSize);
        for (let epoch = 0; epoch < epochs; epoch++) {
          for (let i = 0; i < datasetSize; i += batchSize) {
            const batchIndices = indices.slice(i, i + batchSize);
            const obsBatch = tf.gather(observations, batchIndices);
            const actBatch = tf.gather(actions, batchIndices);
            const advBatch = tf.gather(advantages, batchIndices);
            const retBatch = tf.gather(returns, batchIndices);
            const logpOldBatch = tf.gather(logpOld, batchIndices);
            await this.actorOptimizer.minimize(() => {
              const losses = this.computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch);
              return losses.totalActorLoss;
            });
            await this.criticOptimizer.minimize(() => {
              const losses = this.computeLosses(obsBatch, actBatch, advBatch, retBatch, logpOldBatch);
              return losses.criticLoss;
            });
            obsBatch.dispose(); actBatch.dispose(); advBatch.dispose(); retBatch.dispose(); logpOldBatch.dispose();
          }
        }
      }
      
      computeGAE(rewards, values, dones) {
        let adv = new Array(rewards.length);
        let lastgaelam = 0;
        for (let t = rewards.length - 1; t >= 0; t--) {
          const nextValue = t === rewards.length - 1 ? 0 : values[t+1];
          const nextNonTerminal = dones[t] ? 0 : 1;
          const delta = rewards[t] + this.gamma * nextValue * nextNonTerminal - values[t];
          lastgaelam = delta + this.gamma * this.lam * nextNonTerminal * lastgaelam;
          adv[t] = lastgaelam;
        }
        const returns = adv.map((a, idx) => a + values[idx]);
        const mean = adv.reduce((a, b) => a + b, 0) / adv.length;
        const std = Math.sqrt(adv.map(a => (a - mean) ** 2).reduce((a, b) => a + b, 0) / adv.length);
        adv = adv.map(a => (a - mean) / (std + 1e-8));
        return { advantages: adv, returns: returns };
      }
    }
    
    /*******************************************************
     * Global Setup and Reinitialization
     *******************************************************/
    const canvas = document.getElementById("simulationCanvas");
    const statusDiv = document.getElementById("status");
    
    let simulator;  // instance of ChainPoleSimulator
    let agent;      // PPOAgent instance
    let obs_dim;
    const action_dim = 1;
    
    // Experience buffers (per episode)
    let bufferObservations = [];
    let bufferActions = [];
    let bufferRewards = [];
    let bufferDones = [];
    let bufferLogProbs = [];
    let bufferValues = [];
    
    function initSimulation() {
      const numPoles = parseInt(document.getElementById("numPolesSelect").value);
      simulator = new ChainPoleSimulator(canvas, numPoles);
      obs_dim = 2 + 2 * simulator.numPoles;
      agent = new PPOAgent(obs_dim, action_dim);
      // Clear experience buffers.
      bufferObservations = [];
      bufferActions = [];
      bufferRewards = [];
      bufferDones = [];
      bufferLogProbs = [];
      bufferValues = [];
      statusDiv.textContent = `Configuration applied: ${numPoles} Pole${numPoles > 1 ? "s (chain configuration)" : ""}`;
    }
    initSimulation();
    document.getElementById("applyConfig").addEventListener("click", initSimulation);
    
    /*******************************************************
     * Simulation & Training Loop
     *
     * The physics & training updates run via setInterval
     * (every ~20ms) while rendering uses requestAnimationFrame.
     * In this version, when a termination (e.g. pole touches ground)
     * is detected, the collected episode experience is immediately
     * used to train the agent, the buffers are cleared, and the
     * episode is restarted.
     *******************************************************/
    let lastSimTime = performance.now();
    
    setInterval(() => {
      const now = performance.now();
      const elapsed = now - lastSimTime;
      lastSimTime = now;
      
      // Get current state and decide an action.
      const state = Array.from(simulator.state);
      const { action, logProb, value } = agent.getAction(state);
      const force = action[0];  // single continuous control
      
      // Update the simulation.
      simulator.update(force, elapsed);
      
      // Use constant reward per update.
      const reward = 1;
      const done = simulator.isTerminal() ? 1 : 0;
      
      bufferObservations.push(state);
      bufferActions.push([force]);
      bufferRewards.push(reward);
      bufferDones.push(done);
      bufferLogProbs.push(logProb);
      bufferValues.push(value);
      
      // If episode is terminated, do a training update for this episode.
      if (done === 1) {
        if (bufferObservations.length > 0) {
          const gae = agent.computeGAE(bufferRewards, bufferValues, bufferDones);
          const obsTensor = tf.tensor2d(bufferObservations, [bufferObservations.length, obs_dim]);
          const actTensor = tf.tensor2d(bufferActions, [bufferActions.length, action_dim]);
          const advTensor = tf.tensor1d(gae.advantages);
          const retTensor = tf.tensor1d(gae.returns);
          const logpOldTensor = tf.tensor1d(bufferLogProbs);
          agent.update(obsTensor, actTensor, advTensor, retTensor, logpOldTensor)
               .then(() => {
                 console.log("Episode training update completed.");
                 obsTensor.dispose();
                 actTensor.dispose();
                 advTensor.dispose();
                 retTensor.dispose();
                 logpOldTensor.dispose();
               });
        }
        // Clear experience buffers.
        bufferObservations = [];
        bufferActions = [];
        bufferRewards = [];
        bufferDones = [];
        bufferLogProbs = [];
        bufferValues = [];
        // Restart the episode.
        simulator.reset();
      }
      
      statusDiv.textContent = `Episode Steps: ${bufferObservations.length} (Obs Dim: ${obs_dim})`;
    }, 20);
    
    function renderLoop() {
      simulator.render();
      requestAnimationFrame(renderLoop);
    }
    requestAnimationFrame(renderLoop);
  </script>
</body>
</html>

</html>
